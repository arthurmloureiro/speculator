{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from speculator import SpectrumPCA\n",
    "from speculator import Speculator\n",
    "dtype = tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA basis consutrction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PCA compression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCABasis = SpectrumPCA(n_parameters = , # number of parameters\n",
    "                       n_wavelengths = , # number of wavelength values\n",
    "                       n_pcas = , # number of pca coefficients to include in the basis\n",
    "                       spectrum_filenames = , # list of filenames containing the log spectra for training the PCA\n",
    "                       parameter_filenames = , # list of filenames containing the corresponding parameter values\n",
    "                       parameter_selection = None) # pass an optional function that takes in parameter vector(s) and returns True/False for any extra parameter cuts we want to impose on the training sample (eg we may want to restrict the parameter ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the PCA compression in the following steps:<br/>\n",
    "(1) compute shift and scale for both parameters and spectra (looping over the data files)<br/>\n",
    "(2) compute (incremental) PCA basis for the training data<br/>\n",
    "(3) transform the training data into the PCA basis for subsequent training with neural network<br/>\n",
    "(4) transform a validation set into the PCA basis to interrogate the accuracy of the basis set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCABasis.compute_spectrum_parameters_shift_and_scale() # computes shifts and scales for spectra and parameters\n",
    "\n",
    "PCABasis.train_pca()\n",
    "\n",
    "PCABasis.transform_and_stack_training_data(filename = , # filename = path + prefix for saving stacked training data (both parameters and pca coefficients files)\n",
    "                                           retain=False) # retain = True will keep stacked versions of the training data (both parameters and pca coefficients) as class attributes\n",
    "\n",
    "validation_spectra, validation_spectra_in_pca_basis = PCABasis.validate_pca_basis(spectrum_filename='validation_spectra.txt') # spectrum filename is a file of validation (log) spectra to test the PCA basis out on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract some bits from the PCABasis object for passing to the emulator in a moment (this is really just to show you explicitly which useful bits are now contained in the PCABasis object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transform_matrix = PCABasis.pca_transform_matrix\n",
    "\n",
    "pca_shift = PCABasis.pca_shift\n",
    "pca_scale = PCABasis.pca_scale\n",
    "\n",
    "parameters_shift = PCABasis.parameters_shift\n",
    "parameters_scale = PCABasis.parameters_scale\n",
    "\n",
    "spectrum_shift = PCABasis.spectrum_shift\n",
    "spectrum_scale = PCABasis.spectrum_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training data (parameters and pca coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = tf.convert_to_tensor(np.load('stacked_training_data_parameters.npy').astype(np.float32))\n",
    "training_pca = tf.convert_to_tensor(np.load('stacked_training_data_pca.npy').astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the PCA neural network emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speculator = Speculator(n_parameters = n_parameters, \n",
    "                       wavelengths = , # array of wavelengths\n",
    "                       pca_transform_matrix = pca_transform_matrix, \n",
    "                       parameters_shift = parameters_shift, \n",
    "                       parameters_scale = parameters_scale, \n",
    "                       pca_shift = pca_shift, \n",
    "                       pca_scale = pca_scale, \n",
    "                       spectrum_shift = spectrum_shift, \n",
    "                       spectrum_scale = spectrum_scale, \n",
    "                       n_hidden = [256, 256, 256], \n",
    "                       restore = False, \n",
    "                       restore_filename = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the emulator: example utilizing a cooling schedule with respect to both batch size (increasing) and learning rate (decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooling schedule\n",
    "validation_split = 0.1\n",
    "lr = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "batch_size = [1000, 10000, 50000, int((1-validation_split) * training_theta.shape[0])]\n",
    "gradient_accumulation_steps = [1, 1, 1, 10] # split the largest batch size into 10 when computing gradients to avoid memory overflow\n",
    "\n",
    "# early stopping set up\n",
    "patience = 20\n",
    "\n",
    "# train using cooling/heating schedule for lr/batch-size\n",
    "for i in range(len(lr)):\n",
    "\n",
    "    print('learning rate = ' + str(lr[i]) + ', batch size = ' + str(batch_size[i]))\n",
    "\n",
    "    # set learning rate\n",
    "    speculator.optimizer.lr = lr[i]\n",
    "\n",
    "    # split into validation and training sub-sets\n",
    "    n_validation = int(training_theta.shape[0] * validation_split)\n",
    "    n_training = training_theta.shape[0] - n_validation\n",
    "    training_selection = tf.random.shuffle([True] * n_training + [False] * n_validation)\n",
    "\n",
    "    # create iterable dataset (given batch size)\n",
    "    training_data = tf.data.Dataset.from_tensor_slices((training_theta[training_selection], training_pca[training_selection])).shuffle(n_training).batch(batch_size[i])\n",
    "\n",
    "    # set up training loss\n",
    "    training_loss = [np.infty]\n",
    "    validation_loss = [np.infty]\n",
    "    best_loss = np.infty\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # loop over epochs\n",
    "    while early_stopping_counter < patience:\n",
    "\n",
    "        # loop over batches\n",
    "        for theta, pca in training_data:\n",
    "\n",
    "            # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes)\n",
    "            if gradient_accumulation_steps[i] == 1:\n",
    "                loss = speculator.training_step(theta, pca)\n",
    "            else:\n",
    "                loss = speculator.training_step_with_accumulated_gradients(theta, pca, accumulation_steps=gradient_accumulation_steps[i])\n",
    "\n",
    "        # compute validation loss at the end of the epoch\n",
    "        validation_loss.append(speculator.compute_loss(training_theta[~training_selection], training_pca[~training_selection]).numpy())\n",
    "\n",
    "        # early stopping condition\n",
    "        if validation_loss[-1] < best_loss:\n",
    "            best_loss = validation_loss[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            speculator.update_emulator_parameters()\n",
    "            speculator.save(\"model\")\n",
    "            print('Validation loss = ' + str(best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the trained model and call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speculator = Speculator(restore=True, restore_filename=\"model\")\n",
    "\n",
    "theta = # chose some input parameters to evaluate your (log) spectrum for\n",
    "log_spectrum = speculator.log_spectrum_(theta) # compute log spectrum\n",
    "\n",
    "plt.plot(speculator.wavelengths, log_spectrum)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
