{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from speculator import SpectrumPCA\n",
    "from speculator import Speculator\n",
    "dtype = tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA basis consutrction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PCA compression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCABasis = SpectrumPCA(n_parameters = , # number of parameters\n",
    "                       n_wavelengths = , # number of wavelength values\n",
    "                       n_pcas = , # number of pca coefficients to include in the basis\n",
    "                       log_spectrum_filenames = , # list of filenames containing the (un-normalized) log spectra for training the PCA\n",
    "                       parameter_filenames = , # list of filenames containing the corresponding parameter values\n",
    "                       parameter_selection = None) # pass an optional function that takes in parameter vector(s) and returns True/False for any extra parameter cuts we want to impose on the training sample (eg we may want to restrict the parameter ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the PCA compression in the following steps:<br/>\n",
    "(1) compute shift and scale for both parameters and (log) spectra (looping over the data files)<br/>\n",
    "(2) compute (incremental) PCA basis for the training data<br/>\n",
    "(3) transform the training data into the PCA basis for subsequent training with neural network<br/>\n",
    "(4) transform a validation set into the PCA basis to interrogate the accuracy of the basis set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCABasis.compute_spectrum_parameters_shift_and_scale() # computes shifts and scales for (log) spectra and parameters\n",
    "\n",
    "PCABasis.train_pca()\n",
    "\n",
    "PCABasis.transform_and_stack_training_data(filename = , # filename = path + prefix for saving stacked training data (both parameters and pca coefficients files)\n",
    "                                           retain = True) # retain = True will keep stacked versions of the training data (both parameters and pca coefficients) as class attributes\n",
    "\n",
    "validation_spectra, validation_spectra_in_pca_basis = PCABasis.validate_pca_basis(log_spectrum_filename = # spectrum filename is a file of validation (log) spectra to test the PCA basis out on\n",
    "                                                                                 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training data (parameters and pca coefficients; note that shifts and re-scaling for all inputs/outputs are taken care of internally to the Speculator object so you do not need to shift/scale the training data at this stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_theta = tf.convert_to_tensor(PCABasis.training_parameters.astype(np.float32))\n",
    "training_pca = tf.convert_to_tensor(PCABasis.training_pca.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the PCA neural network emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speculator = Speculator(n_parameters = , # number of model parameters \n",
    "                       wavelengths = , # array of wavelengths\n",
    "                       pca_transform_matrix = PCABasis.pca_transform_matrix, \n",
    "                       parameters_shift = PCABasis.parameter_shift, \n",
    "                       parameters_scale = PCABasis.parameter_scale, \n",
    "                       pca_shift = PCABasis.pca_shift, \n",
    "                       pca_scale = PCABasis.pca_scale, \n",
    "                       log_spectrum_shift = PCABasis.log_spectrum_shift, \n",
    "                       log_spectrum_scale = PCABasis.log_spectrum_scale, \n",
    "                       n_hidden = [256, 256, 256], # network architecture (list of hidden units per layer)\n",
    "                       restore = False, \n",
    "                       restore_filename = # path + filename prefix for saving the model\n",
    "                       optimizer = tf.keras.optimizers.Adam()) # optimizer for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the emulator: example utilizing a cooling schedule with respect to both batch size (increasing) and learning rate (decreasing), and doing gradient accumulation for very large batch sizes (to avoid running out of memory when training on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooling schedule\n",
    "validation_split = 0.1\n",
    "lr = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "batch_size = [1000, 10000, 50000, int((1-validation_split) * training_theta.shape[0])]\n",
    "gradient_accumulation_steps = [1, 1, 1, 10] # split the largest batch size into 10 when computing gradients to avoid memory overflow\n",
    "\n",
    "# early stopping set up\n",
    "patience = 20\n",
    "max_epochs = 1000\n",
    "\n",
    "# train using cooling/heating schedule for lr/batch-size\n",
    "for i in range(len(lr)):\n",
    "\n",
    "    print('learning rate = ' + str(lr[i]) + ', batch size = ' + str(batch_size[i]))\n",
    "\n",
    "    # set learning rate\n",
    "    speculator.optimizer.lr = lr[i]\n",
    "\n",
    "    # split into validation and training sub-sets\n",
    "    n_validation = int(training_theta.shape[0] * validation_split)\n",
    "    n_training = training_theta.shape[0] - n_validation\n",
    "    training_selection = tf.random.shuffle([True] * n_training + [False] * n_validation)\n",
    "\n",
    "    # create iterable dataset (given batch size)\n",
    "    training_data = tf.data.Dataset.from_tensor_slices((training_theta[training_selection], training_pca[training_selection])).shuffle(n_training).batch(batch_size[i])\n",
    "\n",
    "    # set up training loss\n",
    "    training_loss = [np.infty]\n",
    "    validation_loss = [np.infty]\n",
    "    best_loss = np.infty\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # loop over epochs\n",
    "    with trange(max_epochs) as t:\n",
    "        \n",
    "        for epoch in t:\n",
    "\n",
    "            # loop over batches\n",
    "            for theta, pca in training_data:\n",
    "\n",
    "                # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes)\n",
    "                if gradient_accumulation_steps[i] == 1:\n",
    "                    loss = speculator.training_step_pca(pca, theta)\n",
    "                else:\n",
    "                    loss = speculator.training_step_with_accumulated_gradients_pca(pca, theta, accumulation_steps=gradient_accumulation_steps[i])\n",
    "\n",
    "            # compute validation loss at the end of the epoch\n",
    "            validation_loss.append(speculator.compute_loss_pca(training_pca[~training_selection], training_theta[~training_selection]).numpy())\n",
    "\n",
    "            # update the progressbar\n",
    "            t.set_postfix(loss=validation_loss[-1])\n",
    "            \n",
    "            # early stopping condition\n",
    "            if validation_loss[-1] < best_loss:\n",
    "                best_loss = validation_loss[-1]\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                speculator.update_emulator_parameters()\n",
    "                speculator.save(\"model\")\n",
    "                print('Validation loss = ' + str(best_loss))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the trained model and call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speculator = Speculator(restore=True, \n",
    "                        restore_filename= # path + prefix of the saved model file\n",
    "                       )\n",
    "\n",
    "theta = # chose some input parameters to evaluate your (log) spectrum for\n",
    "log_spectrum = speculator.log_spectrum_(theta) # compute log spectrum\n",
    "\n",
    "plt.plot(speculator.wavelengths, log_spectrum)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
